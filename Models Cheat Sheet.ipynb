{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Models Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carliebadder/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression (ordinary least squares)\n",
    "\n",
    "* **Linear Relationship**: there is a linear relationship between the independent and dependent variables\n",
    "* **Multivariate Normality** of all variables: check the data normality with a histogram \n",
    "* **No or Little Multicollinearity**: independent variables should not be correlated because it could cause confusion in interpretability of results, extreme changes in coefficients between only slightly varying models, etc\n",
    "    * Check for using correlation matrix\n",
    "    * Check for using tolerance: T = 1-R^2, multicollinearity if T < 0.01\n",
    "    * Check for using variance inflation factor (VIF): VIF = 1/T, multicollinearity if VIF > 100\n",
    "    * Fix by centering the data OR removing the correlated independent variables\n",
    "* **No Autocorrelation**: residuals should be independent of one another, ie. no periodicity\n",
    "    * Check for using Durbin-Watson test: 1.5 < DW < 2.5 means no autocorrelation\n",
    "* **Homoscedasticity**: residuals should be equally distributed across the regression line\n",
    "    * If heteroscedasticity is present, a non-linear correction could help\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret results values from linear regression\n",
    "* **DF Model**: degrees of freedom of the model; the number of predictor variables\n",
    "* **DF Residuals**: degrees of freedom of the residuals; (# observations) - (DF Model) - 1; want a larger number\n",
    "* **R^2**: cost function called the coefficient of determination that reflects how much variability in the data is captured by the model\n",
    "    * R^2 = 1 - SSE/SST\n",
    "    * SSE = sum of squared errors = SUM (Y_obs - Y_pred)^2 --> randomness left in model\n",
    "    * SST = total sum squares = SUM (Y_obs - Y_mean)^2 --> variance in data\n",
    "* **Adjusted R^2**: more predictor variables typically increase the R^2, but there's the curse of dimensionality! Adjusted R^2 accounts for model complexity/number of predictor variables\n",
    "* **F-Statistic**: you can use hypothesis testing to guide your model, but it's not an absolute indicator of model quality. Null hypothesis of the F-test is that the data can be modeled by setting regression coefficients to zero.\n",
    "    * If p-value of the F-test is < 0.05, reject the null and assume the model is doing something right\n",
    "* **Log-likelihood**: a cost function that is calculated under the assumption that the errors follow a normal distribution (slightly unreasonable)\n",
    "    * Want to maximize this number (maximum likelihood estimator)\n",
    "* **AIC and BIC**: calculate penalties for model complexity based on the log likelihood\n",
    "    * AIC: akaike information criterion; relative estimate of information loss between different models\n",
    "    * BIC: Bayesian information criterion; Bayesian argument not information and more critical than AIC\n",
    "    * Use these to compare between different models - use the model with the lower value\n",
    "* **Coefficients**: or weights\n",
    "* **Standard Error**: metric associated with covariance matrix of estimated coefficients\n",
    "* **t-statistic**: using the t-test for each estimated coefficient\n",
    "    * Null hypothesis: the coefficient of a given predictor is zero, ie. it has no effect on the target variable\n",
    "    * If p-value of t-test < 0.05, reject the null and the coefficient is significant\n",
    "* **Conidence Interval**: range of values we'd expect to find the parameter of interest (ie. the coefficients)\n",
    "    * Want a smaller range because it indicates higher confidence\n",
    "* **Skewness and Kurtosis**:\n",
    "* **Omnibus Test**:\n",
    "* **Durbin-Watson**:\n",
    "* **Jarque-Bera Test**:\n",
    "* **Condition Number**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sm.OLS with statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Via the `statsmodels.api` package\n",
    "* To use this method, you need to generate a **matrix** of **features**, **`X`** and a **vector** of **targets**, **`y`** where each row represents a single **observation**.  In statsmodels, you can do this with a call to **`patsy.dmatrices`**\n",
    "* Given a dataframe with the columns: Y, X1, X2, X3, X4, X5, X6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import patsy\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create your feature matrix (X) and target vector (y)\n",
    "y, X = patsy.dmatrices('Y ~ X1 + X2 + X3 + X4 + X5 + X6', data=df, return_type=\"dataframe\")\n",
    "# Create your model\n",
    "model = sm.OLS(y, X)\n",
    "# Fit your model to your training set\n",
    "fit = model.fit()\n",
    "# Print summary statistics of the model's performance\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smf.OLS with statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### `statsmodels.formula.api`\n",
    "The formula approach handles the creation of the `X` and `y` matrices internally, so all you have to do is supply the R formula for your model when you create your `ols` object.  \n",
    "* Given a dataframe with the columns: Y, X1, X2, X3, X4, X5, X6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "lm1 = smf.ols('Y ~ X1 + X2 + X3 + X4 + X5 + X6', data=df)\n",
    "# Fit the model\n",
    "fit1 = lm1.fit()\n",
    "# Print summary statistics of the model's performance\n",
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Residuals\n",
    "* residuals: (actual value - predicted value)\n",
    "* want errors to be random \n",
    "* If residuals look systematic (e.g. missing high for one range and low for another) then we probably are missing the actual functional dependency underlying the data (perhaps it's not really linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use statsmodels to plot the residuals\n",
    "fit1.resid.plot(style='o', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LinearRegression() with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* with dataframe df.columns = Y, X2, X3, X4, X5, X6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create an empty model\n",
    "lr = LinearRegression()\n",
    "# Choose the predictor variables, here all but the first which is the response variable\n",
    "# This model is analogous to the Y ~ X1 + X2 + X3 + X4 + X5 + X6 model\n",
    "X = df.iloc[:, 1:]\n",
    "# Choose the response variable(s)\n",
    "y = df.iloc[:, 0]\n",
    "# Fit the model to the full dataset\n",
    "lr.fit(X, y)\n",
    "# Print out the R^2 for the model against the full dataset\n",
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* using a section of the dataframe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create an empty model\n",
    "lr1 = LinearRegression()\n",
    "# Choose the predictor variables, here all but the first which is the response variable\n",
    "# This model is analogous to the Y ~ X1 + X3 + X6 model\n",
    "X = df[['X1', 'X3', 'X6']]\n",
    "# Choose the response variable(s)\n",
    "y = df['Y']\n",
    "# Fit the model to the full dataset\n",
    "lr1.fit(X, y)\n",
    "# Print out the R^2 for the model against the full dataset\n",
    "lr1.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Output from LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- `fit()`: Fit a model to a set of training data\n",
    "\n",
    "** After model is fit **\n",
    "- `score()`: Score the performance of a model on a given sample of data with known ground truth dependent variables\n",
    "- `predict()`: Predict target/response variables based on a sample of independent variables (features, predictors, etc)\n",
    "- `intercept_`: our $\\beta_0$ intercept in our regression model\n",
    "- `coef_`: the other $\\beta$s in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr.score(X_train, y_train) # the R^2 of the model on the whole training set\n",
    "lr.score(X_test, y_test) # the R^2 of the model for the predicted values of the test set against the actual y_test\n",
    "\n",
    "y_predicted = lr.predict(X_test)\n",
    "\n",
    "lr.intercept_\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PolynomialRegression() with sklearn and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can transform the predictor variables by any type of function we want before inputting them to linear regression.  This is the idea behind [**Polynomial Regression**](https://en.wikipedia.org/wiki/Polynomial_regression) and it allows us (along with similar functional regressions) to essentially model our response variables as any function of our predictor variables that we like.  Viewed in this way, Linear Regression is just a special instance of Polynomial Regression with a polynomial of degree 1.\n",
    "\n",
    "** Pipelines **\n",
    "The [***make_pipeline***](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) function is used to string together a pipeline of operations that is able to first transform our linear features into polynomial features and then run a linear regression against the resulting polynomial features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import PolynomialFeatures and make_pipeline for Polynomial Regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set the degree of our polynomial\n",
    "degree = 3\n",
    "# Generate the model type with make_pipeline\n",
    "# This tells it the first step is to generate 3rd degree polynomial features in the input features and then run\n",
    "# a linear regression on the resulting features\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "# Fit our model to the training data\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "# Ground Truth\n",
    "X_test, y_test\n",
    "\n",
    "# Prediction\n",
    "X_test, est.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visualizing pairwise correlations with seaborn\n",
    "sns.pairplot(example_df[['column1','column2', 'column3', 'column4', 'column5']])\n",
    "\n",
    "# plots correlations as scatter plots of each column vs each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variables with Patsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things:  \n",
    "1) First we'll add an intercept (comes for free w/ patsy)  \n",
    "2) Second, in this case, one 'make' ('alfo-romero' here) is missing from the matrix\n",
    "\n",
    "This is because patsy knows about (The Dummy Variable Trap)[http://www.algosome.com/articles/dummy-variable-trap-regression.html] The idea behind DVT: Take a categorical variable that has two outcomes (example: boy & girl for the feature 'sex) - We only need one column: \"Girl\", we can obviously solve for 'Boy' given this column -- if we had both columns, we would automatically introduce multicolinearity  -- this idea can be extended to categorical features with 20+ categories ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use patsy to convert the feature 'make' to categorical variables\n",
    "\n",
    "X=patsy.dmatrix('make',data=df,return_type='dataframe')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - K-Nearest Neighbors\n",
    "\n",
    "* lazy\n",
    "* 'fits' fast, predicts slow (need to store all the original training data)\n",
    "* assuming d-dimensional data, the straightforward implementation is O(dn) time\n",
    "* higher memory (saves training set)\n",
    "* various implementations, including weighted KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaling is crucial to KNN\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.4, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 3-fold cross validation with\n",
    "# n_neighbors = 5 and print out the scores and their means\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X_train, y_train, \n",
    "                         cv=3, scoring='accuracy')\n",
    "\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-validation to find optimal k-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for an optimal value of K for KNN\n",
    "\n",
    "k_scores = []\n",
    "\n",
    "for k in range(1, 31):\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    \n",
    "    k_scores.append((k, scores.mean()))\n",
    "    \n",
    "k_scores = pd.DataFrame(k_scores, columns=['k', 'accuracy']).set_index('k')\n",
    "k_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once the optimal k-value is found, re-evaluate the model and run the model on the test data set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning with GridSearchCV and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "# Grid search uses all the parameters\n",
    "\n",
    "param_grid = {'n_neighbors': range(1, 31)}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), \n",
    "                    param_grid, \n",
    "                    cv=3, \n",
    "                    scoring='accuracy')\n",
    "\n",
    "grid = grid.fit(X_train, y_train)\n",
    "\n",
    "# extensive results provided\n",
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=3)\n",
    "svd_out = svd.fit_transform(user_item_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "699px",
    "left": "0px",
    "right": "1071.22px",
    "top": "107px",
    "width": "417px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
